{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taehwan2/hanghaeAI/blob/main/DistilBERT%EC%8B%AC%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
        "\n",
        "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
      ],
      "metadata": {
        "id": "sbgz49PvHhLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LqgujQUbv6X",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
      ],
      "metadata": {
        "id": "6YP3FxG9IF7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"thedevastator/unlocking-language-understanding-with-the-multin\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "pljEJGuDwx8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lGiZUoPby6e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
        "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBERT의 tokenizer를 불러왔으면 이제 `collate_fn`과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
      ],
      "metadata": {
        "id": "Cvfl_uFLIMWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_data(path, nrows=None):\n",
        "  df = pd.read_csv(path, nrows=nrows, keep_default_na=False)\n",
        "  data = []\n",
        "  for _, row in df.iterrows():\n",
        "    if len(row['premise']) * len(row['hypothesis']) != 0:\n",
        "      data.append({'premise': row['premise'], 'hypothesis': row['hypothesis'], 'label': row['label']})\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "train_data = load_data(path + '/train.csv', nrows=1000)\n",
        "test_data = load_data(path + '/validation_matched.csv', nrows=1000)"
      ],
      "metadata": {
        "id": "btnXN695w2l1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['premise'] + row['hypothesis'])\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data, batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_data, batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
      ],
      "metadata": {
        "id": "bF34XkoYIeEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
        "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
        "\n",
        "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
      ],
      "metadata": {
        "id": "uh-tqY8WInQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW7ETZQzzNp2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "    self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)['last_hidden_state']\n",
        "    x = self.classifier(x[:, 0])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 같이 `TextClassifier`의 `encoder`를 불러온 DistilBERT, 그리고 `classifier`를 linear layer로 설정합니다.\n",
        "그리고 `forward` 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
        "\n",
        "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
      ],
      "metadata": {
        "id": "_hFvSis0JLju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
        "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
        "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
        "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
        "\n",
        "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
      ],
      "metadata": {
        "id": "hU7BWEbgJeKm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "2ba82ac2-65f6-4310-af7b-db21dcf970de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 17.51239264011383\n",
            "Epoch   1 | Train Loss: 17.373963832855225\n",
            "Epoch   2 | Train Loss: 17.33500647544861\n",
            "Epoch   3 | Train Loss: 17.22771990299225\n",
            "Epoch   4 | Train Loss: 17.132736086845398\n",
            "Epoch   5 | Train Loss: 16.957014560699463\n",
            "Epoch   6 | Train Loss: 16.862652897834778\n",
            "Epoch   7 | Train Loss: 16.926201343536377\n",
            "Epoch   8 | Train Loss: 16.894115805625916\n",
            "Epoch   9 | Train Loss: 16.796897888183594\n",
            "Epoch  10 | Train Loss: 16.855460166931152\n",
            "Epoch  11 | Train Loss: 16.750187277793884\n",
            "Epoch  12 | Train Loss: 16.622865319252014\n",
            "Epoch  13 | Train Loss: 16.728083729743958\n",
            "Epoch  14 | Train Loss: 16.539232850074768\n",
            "Epoch  15 | Train Loss: 16.556726038455963\n",
            "Epoch  16 | Train Loss: 16.40929800271988\n",
            "Epoch  17 | Train Loss: 16.47456306219101\n",
            "Epoch  18 | Train Loss: 16.42001336812973\n",
            "Epoch  19 | Train Loss: 16.29828578233719\n",
            "Epoch  20 | Train Loss: 16.219700694084167\n",
            "Epoch  21 | Train Loss: 16.21201992034912\n",
            "Epoch  22 | Train Loss: 16.418890058994293\n",
            "Epoch  23 | Train Loss: 16.22357338666916\n",
            "Epoch  24 | Train Loss: 16.215638279914856\n",
            "Epoch  25 | Train Loss: 16.139372050762177\n",
            "Epoch  26 | Train Loss: 16.014805495738983\n",
            "Epoch  27 | Train Loss: 15.983869135379791\n",
            "Epoch  28 | Train Loss: 16.133232176303864\n",
            "Epoch  29 | Train Loss: 16.011729061603546\n",
            "Epoch  30 | Train Loss: 16.149725794792175\n",
            "Epoch  31 | Train Loss: 16.00684255361557\n",
            "Epoch  32 | Train Loss: 15.954668641090393\n",
            "Epoch  33 | Train Loss: 15.943844020366669\n",
            "Epoch  34 | Train Loss: 16.06774240732193\n",
            "Epoch  35 | Train Loss: 15.953875303268433\n",
            "Epoch  36 | Train Loss: 15.838007152080536\n",
            "Epoch  37 | Train Loss: 15.94975882768631\n",
            "Epoch  38 | Train Loss: 15.750451564788818\n",
            "Epoch  39 | Train Loss: 15.853711545467377\n",
            "Epoch  40 | Train Loss: 15.810093581676483\n",
            "Epoch  41 | Train Loss: 15.7865309715271\n",
            "Epoch  42 | Train Loss: 15.726392567157745\n",
            "Epoch  43 | Train Loss: 15.557546973228455\n",
            "Epoch  44 | Train Loss: 15.772082388401031\n",
            "Epoch  45 | Train Loss: 15.701775908470154\n",
            "Epoch  46 | Train Loss: 15.599506378173828\n",
            "Epoch  47 | Train Loss: 15.59752494096756\n",
            "Epoch  48 | Train Loss: 15.602905690670013\n",
            "Epoch  49 | Train Loss: 15.499629497528076\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda').long()\n",
        "\n",
        "    preds = model(inputs)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DjphVwXL00E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2b3d681-2029-42e0-a501-8547988ffe32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========> Train acc: 0.549 | Test acc: 0.397\n"
          ]
        }
      ],
      "source": [
        "def accuracy(model, dataloader):\n",
        "    cnt = 0      # 전체 샘플 수\n",
        "    acc = 0      # 정답 개수 누적\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)  # 로짓(logit) 출력\n",
        "\n",
        "        # 시그모이드는 생략 가능 (BCEWithLogitsLoss를 썼다면 threshold만 적용)\n",
        "        preds = torch.argmax(preds, dim=-1)\n",
        "        #preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "        cnt += labels.shape[0]  # 총 샘플 수 누적\n",
        "        acc += (labels == preds).sum().item()  # 예측이 맞은 수 누적\n",
        "\n",
        "    return acc / cnt  # 정확도 반환\n",
        "\n",
        "# 평가 시 gradient 계산 비활성화\n",
        "with torch.no_grad():\n",
        "    model.eval()  # 평가 모드로 전환 (계산 비활성화)\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 사전학습 된거 가중치 제거"
      ],
      "metadata": {
        "id": "wKqVXphl_vHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class TextClassifier2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전학습된 DistilBERT 모델을 encoder로 불러옴\n",
        "        self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "        # 모든 가중치 초기화\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        # DistilBERT의 모든 파라미터를 랜덤 초기화\n",
        "        for module in self.encoder.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)  # Xavier 초기화\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                nn.init.uniform_(module.weight, -0.1, 0.1)  # 임베딩 초기화\n",
        "\n",
        "        # 분류기의 가중치도 초기화\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)['last_hidden_state']\n",
        "        x = self.classifier(x[:, 0])\n",
        "        return x\n",
        "\n",
        "# 모델 생성\n",
        "model2 = TextClassifier2()\n"
      ],
      "metadata": {
        "id": "1BYCu_Mb8Sjw",
        "outputId": "1964794b-12d0-4d4f-afab-53104679a80d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model2.encoder.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "bJZx4BtH8Vit"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model2 = model2.to('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model2.parameters(), lr=lr)\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model2.train()\n",
        "  for data in train_loader:\n",
        "    model2.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda').long()\n",
        "\n",
        "    preds = model2(inputs)\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
      ],
      "metadata": {
        "id": "HYhvt2xZ8S9V",
        "outputId": "89c14b66-7cfd-4b56-994a-a107a9008fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 18.11597728729248\n",
            "Epoch   1 | Train Loss: 17.97783398628235\n",
            "Epoch   2 | Train Loss: 17.89479374885559\n",
            "Epoch   3 | Train Loss: 17.576369166374207\n",
            "Epoch   4 | Train Loss: 17.628673791885376\n",
            "Epoch   5 | Train Loss: 18.12171733379364\n",
            "Epoch   6 | Train Loss: 17.864755988121033\n",
            "Epoch   7 | Train Loss: 17.660717368125916\n",
            "Epoch   8 | Train Loss: 17.61660075187683\n",
            "Epoch   9 | Train Loss: 17.635198831558228\n",
            "Epoch  10 | Train Loss: 17.56428360939026\n",
            "Epoch  11 | Train Loss: 17.80129587650299\n",
            "Epoch  12 | Train Loss: 17.606622219085693\n",
            "Epoch  13 | Train Loss: 17.527899861335754\n",
            "Epoch  14 | Train Loss: 17.646907567977905\n",
            "Epoch  15 | Train Loss: 17.522242784500122\n",
            "Epoch  16 | Train Loss: 17.705416440963745\n",
            "Epoch  17 | Train Loss: 17.438100814819336\n",
            "Epoch  18 | Train Loss: 17.551884174346924\n",
            "Epoch  19 | Train Loss: 17.564248919487\n",
            "Epoch  20 | Train Loss: 17.431082725524902\n",
            "Epoch  21 | Train Loss: 17.392606258392334\n",
            "Epoch  22 | Train Loss: 17.447736859321594\n",
            "Epoch  23 | Train Loss: 17.363917112350464\n",
            "Epoch  24 | Train Loss: 17.552752137184143\n",
            "Epoch  25 | Train Loss: 17.753161311149597\n",
            "Epoch  26 | Train Loss: 17.632689833641052\n",
            "Epoch  27 | Train Loss: 17.628012537956238\n",
            "Epoch  28 | Train Loss: 17.42620360851288\n",
            "Epoch  29 | Train Loss: 17.5056391954422\n",
            "Epoch  30 | Train Loss: 17.32244074344635\n",
            "Epoch  31 | Train Loss: 17.756445169448853\n",
            "Epoch  32 | Train Loss: 17.62270212173462\n",
            "Epoch  33 | Train Loss: 17.60746932029724\n",
            "Epoch  34 | Train Loss: 17.2215176820755\n",
            "Epoch  35 | Train Loss: 17.376721739768982\n",
            "Epoch  36 | Train Loss: 17.369139075279236\n",
            "Epoch  37 | Train Loss: 17.29202175140381\n",
            "Epoch  38 | Train Loss: 17.32550072669983\n",
            "Epoch  39 | Train Loss: 17.192320346832275\n",
            "Epoch  40 | Train Loss: 17.2884019613266\n",
            "Epoch  41 | Train Loss: 17.447273015975952\n",
            "Epoch  42 | Train Loss: 17.363546013832092\n",
            "Epoch  43 | Train Loss: 17.42484152317047\n",
            "Epoch  44 | Train Loss: 17.66979467868805\n",
            "Epoch  45 | Train Loss: 17.64828109741211\n",
            "Epoch  46 | Train Loss: 17.19129168987274\n",
            "Epoch  47 | Train Loss: 17.229745030403137\n",
            "Epoch  48 | Train Loss: 17.47087836265564\n",
            "Epoch  49 | Train Loss: 17.22138023376465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, dataloader):\n",
        "    cnt = 0      # 전체 샘플 수\n",
        "    acc = 0      # 정답 개수 누적\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)  # 로짓(logit) 출력\n",
        "\n",
        "        # 시그모이드는 생략 가능 (BCEWithLogitsLoss를 썼다면 threshold만 적용)\n",
        "        preds = torch.argmax(preds, dim=-1)\n",
        "        #preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "        cnt += labels.shape[0]  # 총 샘플 수 누적\n",
        "        acc += (labels == preds).sum().item()  # 예측이 맞은 수 누적\n",
        "\n",
        "    return acc / cnt  # 정확도 반환\n",
        "\n",
        "# 평가 시 gradient 계산 비활성화\n",
        "with torch.no_grad():\n",
        "    model.eval()  # 평가 모드로 전환 (계산 비활성화)\n",
        "    train_acc = accuracy(model2, train_loader)\n",
        "    test_acc = accuracy(model2, test_loader)\n",
        "\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "Foks5u95ZQ1_",
        "outputId": "c3700876-2bef-4cfa-f783-224d8676266a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========> Train acc: 0.328 | Test acc: 0.326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###사전학습된 가중치를 제거했을 때 정확도가 차이가심하다"
      ],
      "metadata": {
        "id": "rsFUAQHsALKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpSDdGyDALBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}